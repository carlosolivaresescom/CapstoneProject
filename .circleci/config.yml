version: 2.1

commands:
  destroy-cluster:
    description: Destroy cluster.
    steps:
      - run:
          name: Destroy cluster
          when: on_fail
          command: |
            echo "Destroying cluster ${CIRCLE_WORKFLOW_ID:0:7}"
            export clustername="capstone-${CIRCLE_WORKFLOW_ID:0:7}"
            echo ${clustername}
            sed -e "s/clustername/$clustername/g" .circleci/files/create.yml | eksctl delete cluster -f -
  revert-docker-image:
    description: Revert docker image.
    steps:
      - run:
          name: Destroy cluster
          when: on_fail
          command: |
            echo "Reverting docker image"
            docker run hello-world
            docker login -u ${DOCKER_USER} -p ${DOCKER_PASSWORD}
            docker pull ${DOCKER_USER}/capstoneproject:old
            docker tag ${DOCKER_USER}/capstoneproject:old ${DOCKER_USER}/capstoneproject:latest
            docker push ${DOCKER_USER}/capstoneproject
      - run:
          name: restart cluster
          when: on_fail
          command: |
            export cluster=$(aws eks list-clusters | python3 -c "import sys, json; print(json.load(sys.stdin)['clusters'][0])")
            echo ${cluster}
            aws eks update-kubeconfig --name ${cluster}
            export KUBECONFIG=/root/.kube/config
            kubectl rollout restart deployment.apps/kubeapp-ecr-deploy
            sleep 1m
            kubectl get all

jobs:
  eslint:
    docker:
      - image: circleci/node:14
    steps:
      - checkout
      - run:
          name: lint
          command: |
            cd backend2
            npm install
            npx eslint
            npx eslint app.js

  build:
    docker:
      - image: circleci/node:13.8.0
    steps:
      - checkout
      - restore_cache:
          keys: [build]
      - run:
          name: Build
          command: |
            cd backend2
            npm install
            npm run build

  upload_docker_image:
    machine: true
    steps:
      - checkout
      - run:
          name: build images
          command: |
            cd backend2
            docker build -t capstoneproject .
            docker images
      - run:
          name: push to docker hub while keeping old image
          command: |
            docker login -u ${DOCKER_USER} -p ${DOCKER_PASSWORD}
            docker pull ${DOCKER_USER}/capstoneproject:latest
            docker tag ${DOCKER_USER}/capstoneproject:latest ${DOCKER_USER}/capstoneproject:old
            docker push ${DOCKER_USER}/capstoneproject:old
            docker tag capstoneproject ${DOCKER_USER}/capstoneproject:latest
            docker push ${DOCKER_USER}/capstoneproject

  create-cluster:
    docker:
      - image: alpine/k8s:1.15.12
    steps:
      - checkout
      - run:
          name: Create cluster
          command: |
            export clustername="capstone-${CIRCLE_WORKFLOW_ID:0:7}"
            echo ${clustername}
            sed -e "s/clustername/$clustername/g" .circleci/files/create.yml | eksctl create cluster -f -
            #eksctl create cluster -f .circleci/files/create.yml
      - run:
          name: Get pods
          command: |
            kubectl get pods
      - setup_remote_docker:
          docker_layer_caching: true
      - run:
          name: Run kubectl
          command: |
            kubectl create -f .circleci/files/deployment.yml
            kubectl create -f .circleci/files/ecrserver.yml
            sleep 1m
            #Manually open port 30072
            kubectl get all
      - destroy-cluster

  restart-cluster:
    docker:
      - image: alpine/k8s:1.15.12
    steps:
      - checkout
      - run:
          name: store cluster name
          command: |
            export cluster=$(aws eks list-clusters | python3 -c "import sys, json; print(json.load(sys.stdin)['clusters'][0])")
            echo ${cluster}
            aws eks update-kubeconfig --name ${cluster}
            export KUBECONFIG=/root/.kube/config
            kubectl get svc --all-namespaces
            kubectl config view

      - run:
          name: Restart deployment
          command: |
            kubectl get all
            kubectl rollout restart deployment.apps/kubeapp-ecr-deploy
            sleep 1m
            kubectl get all

  smoke-test:
    machine: true
    steps:
      - checkout
      - run:
          name: Smoke test.
          command: |
            curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl"
            chmod +x kubectl
            mkdir -p ~/.local/bin/kubectl
            mv ./kubectl ~/.local/bin/kubectl
            PATH=$PATH:~/.local/bin/kubectl
            kubectl version --client
            #for new cluster change url
            URL="http://ec2-44-192-83-90.compute-1.amazonaws.com:30072"            
            echo ${URL} 
            if curl -s ${URL} | grep "Success"
            then
              echo "url working"
            else
              echo "url not working"
              exit 1
            fi
      - revert-docker-image

  test:
    docker:
      - image: amazon/aws-cli
    steps:
      - checkout
      - run:
          name: Smoke test.
          command: |
            yum install -y yum-utils \
            device-mapper-persistent-data \
            lvm2 
            # it will setup repo list for you!
            sudo yum-config-manager \
            --add-repo \
            https://download.docker.com/linux/centos/docker-ce.repo
            # Make sure that you have stable one always
            sudo yum-config-manager --enable docker-ce-edge

            # And Finally install it
            sudo yum install docker-ce
            docker --version
            aws eks list-clusters
            cat \<<EOF | tee /etc/yum.repos.d/kubernetes.repo
            [kubernetes]
            name=Kubernetes
            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
            enabled=1
            gpgcheck=1
            repo_gpgcheck=1
            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
            EOF
            yum install -y kubectl
            kubectl version --client
            #export cluster=$(aws eks list-clusters | python3 -c "import sys, json; print(json.load(sys.stdin)['clusters'][0])")
            #echo ${cluster}
            #aws eks update-kubeconfig --name ${cluster}

workflows:
  default:
    jobs:
      - test:
          filters:
            branches:
              only:
                - main
                - createcluster
